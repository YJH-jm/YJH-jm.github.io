---
layout: single
title:  "StyleTTS" # 글 제목
categories: TTS # category
tags: [] # tag
toc: true # 오른쪽의 table of contents
toc_sticky: true # 스크롤 내려도 toc 고정
toc_label: 목차
author_profile: false
# sidebar: # 왼쪽 사이드 바
    # nav: "docs"
search: true # 글 검색 안됨
use_math: true
typore-root-url: ../

---

****

1. [StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis](https://arxiv.org/pdf/2205.15439)
2. [Monotonic alignment](https://velog.io/@castlechoi/Monotonic-alignment)

<br><br>

# StyleTTS





## Architecture



StyleTTS의 목적은 입력으로 들어가는 phonemes (음소) $\mathbf{t}$


$$
\mathbf{t} \in
$$




8개의 모듈이 3개의 주요 카테고리로 나누어져 있음

1. Speech generation module
   - Text encoder
   - Style encoder
   - Decoder
2. TTS prediction module
   - Duration predictor
   - Prosody predictor
3. Utility module (학습에만 이용)
   - Discriminator
   - Text Aligner
   - Pitch extractor



### Text Encoder $T$

Phonemes $\boldsymbol{t}$ 를 인코딩하여  $\boldsymbol{h}_{\text {text }}=T(\boldsymbol{t})$ 

3개의 CNN과 bidirectional LSTM로 구성

<br><br>

### Text Aligner $A$

Mel-spectogram과 phoneme 사이의 alignment $\boldsymbol{d}_{\text {align }}$ 을 생성하는 부분

Decoder $G$가 mel-spectogram을 복원하는 단계에서 같이 학습 진행 (학습 1 단계)

Tacotron2 모델의 decoder의 attention 부분을 모델로하여 text aligner 구성



처음에는 LibriSpeech corpus 데이터 셋을 이용하여 ASR (Automatic Speech Recognition) task에서  $A$를 학습한 후, 

decoder를 학습할 때 TTS task에 맞게 fine-tuning 을 진행

이런 방식으로 진행한 alinger를 Transferable Monotonic Aligner (TMA)  라고 함

<br><br>

### Style Encoder $E$

Melspectogram $\boldsymbol{x}$ 이 주어지면 style encoder를 통해 style vector $s=E(\boldsymbol{x})$를 구할 수 있음

Style Encoder는 4개의 residual block과 average pooling으로 구성되어 있고 시간 축에 적용됨

다양한 reference 오디오를 기반으로 다양한 style representation을 생성할 수 있고 이는 decoder 가 reference 오디오  $\boldsymbol{x}$의 스타일을 반영한 음성을 생성할 수 있음 

<br><br>

### Pitch Extractor $F$

Pitch Extractor를 통해 pitch $\text { F0 }$를  추가적인 과정 없이 Hz 단위로 직접적으로 구하여 직관적인 representation을 제공하고, 음성의 pitch를 더 잘 조절할 수 있게 함

-  $\text { F0 }$는 기본 주파수를 의미하고, 음성의 높낮이를 결정하는 중요한 요소, Hz는 음성의 높낮이 정량적 표현 단위

더 정확하게 예측하기 위해서 end-to-end로 decoder $G$와 같이 학습

Pitch extractor $F$로 LibriSpeech 데이터셋에서 YIN 알고리즘을 통해 추정된 F0 데이터를 ground-truth로 사용하여 pre-trained 한 JDC Network을 사용

- JDC 네트워크는 다양한 음성 특징을 동시에 추출하고 분류하는 데 사용되는 신경망 구조
- YIN 알고리즘은 기본 주차수를 추정하는 알고리즘 중 하나 

Decoder가 학습할 때 fine-tuning 되어 pitch $p_{\boldsymbol{x}}=F(\boldsymbol{x})$ 를 예측하여 melspectogram의 복원하는데 적용

<br><br>

### Decoder $G$

입력으로 들어온 mel-spectogram $\boldsymbol{x}$를 복원 $\hat{\boldsymbol{x}}$ 하도록 학습


$$
\hat{\boldsymbol{x}}=G\left(\boldsymbol{h}_{\mathrm{text}} \cdot \boldsymbol{d}_{\mathrm{align}}, \boldsymbol{s}, p_{\boldsymbol{x}}, n_{\boldsymbol{x}}\right)
$$


- $$\boldsymbol{h}_{\text {text }} \cdot \boldsymbol{d}_{\text {align }}$$ : align된 입력 phoneme의 representation
- $$\boldsymbol{s}$$ : style vector
- $$p_{\boldsymbol{x}}, n_{\boldsymbol{x}}$$ : $$\boldsymbol{x}$$의 각 프레임 마다의 pitch와 energy  



Decoder는 AdaIN을 포함한 7개의 블록으로 구성

중요한 feature가 사라지는 것을 막기 위해 pitch $p_{\boldsymbol{x}}$, energy $n_{\boldsymbol{x}}$, residual phoneme feature $\boldsymbol{h}_{\text {res }}$을 concat 하여 AdaIN 이후의 residual block에 전달

<br><br>

### Discriminator $D$

Decoder가 조금 더 좋은 품질의 음성을 만들수 있도록 하기 위해 사용

Style Encoder와 같은 아키텍쳐를 공유

<br><br>

### Duration Predictor 

AdaLN이 적용된 3개의 bidirectional LSTM $R$ 과 한 개의 linear projection $L$로 구성

$R$은 prosody predictor $P$와 공유되는데, 이는 $R$의 출력 $$\boldsymbol{h}_{\text {prosody }}=R\left(\boldsymbol{h}_{\text {text }}\right)$$가 prosody predictor $P$의 입력으로 들어간다는 의미

<br><br>

### Prosody Predictor $P$

Style vector와 text 정보와 함께  Pitch $\hat{p}_{\boldsymbol{x}}$와 energy $\hat{n}_{\boldsymbol{x}}$ 를 추정하는 역할을 함

Shared bidirectional LSTM $R$을 통해 생성된 $$\boldsymbol{h}_{\text {prosody }}$$ 와 $\boldsymbol{a}$ 을 결합이 되고$\boldsymbol{h}_{\text {prosody }} \cdot \boldsymbol{a}$ 이 값이 residual block과 AdaIN 세트들을 지나 linear projection layer를 통과하여 pitch 와 energy output 생성



<br><br>

## Training

2 stage로 학습을 나누어 진행

첫 번째는 text, pitch, energy, style 로부터 mel-spectogram을 복원하는 과정을 학습하고 

두 번째에서는  duration predictor와 prosody predictor를 제외한 모듈은 고정하고 주어진 text에서 pitch, enegy, style을 복원하는 과정을 학습



### First Stage

#### Mel reconstruction

Mel-spectogram과 이에 대응하는 text가 주어질 때, decoder를 다음과 같이 $L_{1}$를 이용하여 학습


$$
\mathcal{L}_{\text {mel }}=\mathbb{E}_{\boldsymbol{x}, \boldsymbol{t}}\left[\left\|\boldsymbol{x}-G\left(\boldsymbol{h}_{\text {text }} \cdot \boldsymbol{a}_{\text {align }}, \boldsymbol{s}, p_{\boldsymbol{x}}, n_{\boldsymbol{x}}\right)\right\|_{1}\right]
$$
 

Decoder와 text aligner를 E2E 학습 하기 위해 시간을 기준으로 50%-50% 학습 전략을 취함

- 데이터 하나의 시간을 의미하는 것 같음??

시간을 기준으로 절반은 attention의 결과를 사용하여 text aligner 학습 (back propagation)

- 텍스트가 음성으로 변환되는 일반적인 패턴 학습

나머지 절반은 미분이 불가능한 monotonic한 alignment를 사용하는 전략

- 동적 프로그래밍으로 최적의 정합을 찾음

- decoder가 추론하는 동안 monotonic hard alignment 를 통해 더 이해가능한 음성을 생성하도록 학습 가능





참고로 Monotonic alignment에 대한 설명은 아래와 같음

- **Monotonic** alignment
  - 텍스트와 음성 사이의 대응이 순차적으로 발생
  - 텍스트의 첫 번째 음소는 음성의 처음 부분에, 두 번째 음소는 그 다음 부분에 대응하는 식으로, 순서가 보존되면서 정렬이 구성 
  - 음소에 대응하는 음성 구간이 뒤로 갈수록 시간적으로 뒤따라오는 방식
- Hard Monotonic alignment
  - 음소가 특정 음성 프레임과 명확하게 연결되며, 이 연결이 확률적으로 퍼지지 않고 명확하게 결정 
  - 특정 음소가 특정 시간 프레임에 단일하게 대응되며, 다른 시간 프레임에는 대응하지 않음



이런 방법으로 pre-trained 된 text aligner를 fine-tuning 하면 최적의 alignment를 찾을 수 있어 음성의 품질이 향상

 



#### TMA 

E2E로 text aligner를 학습하는 동안 attention alignment의 성능이 올바르게 유지되고 있는지 확인하기 위해 원래의 sequence-to-sequence ASR의 objective인 $\mathcal{L}_{\mathrm{s} 2 \mathrm{s}}$ 적용


$$
\mathcal{L}_{\mathrm{s} 2 \mathrm{s}}=\mathbb{E}_{\boldsymbol{x}, \boldsymbol{t}}\left[\sum_{i=1}^{N} \mathbf{C E}\left(\boldsymbol{t}_{i}, \hat{\boldsymbol{t}}_{i}\right)\right]
$$


- $N$ :  phoneme 의 수 







alignment가 반드시 monotonic 하지는 않기 때문에 soft attention alignment가 monotonic 에 가까워지도록 학습하기 위해 간단한 $L1$ loss 사용 


$$
\mathcal{L}_{\text {mono }}=\mathbb{E}_{\boldsymbol{x}, \boldsymbol{t}}\left[\left\|\boldsymbol{a}_{\mathrm{align}}-\boldsymbol{a}_{\mathrm{hard}}\right\|_{1}\right]
$$


- $$\boldsymbol{a}_{\mathrm{align}}$$ : Text aligner의 결과
- $$\boldsymbol{a}_{\mathrm{hard}}$$ : 동적 프로그래밍의 결과로 얻어진 monotonic hard alignment



<br><br>

#### Adversarial objectives

2개의 adversarial loss 사용 



첫 번째는 adversarial training 을 위한 cross entropy loss $$\mathcal{L}_{\text {adv }}$$


$$
\mathcal{L}_{\text {adv }}=\mathbb{E}_{\boldsymbol{x}, \boldsymbol{t}}[\log D(\boldsymbol{x})+\log (1-D(\hat{\boldsymbol{x}}))]
$$


- $\hat{\boldsymbol{x}}$ : Decoder의 결과로 생성된 mel-spectogram





두 번째는 feature-matching loss $$\mathcal{L}_{\mathrm{fm}}$$


$$
\mathcal{L}_{\mathrm{fm}}=\mathbb{E}_{\boldsymbol{x}, t}\left[\sum_{l=1}^{T} \frac{1}{N_{l}}\left\|D^{l}(\boldsymbol{x})-D^{l}(\hat{\boldsymbol{x}})\right\|_{1}\right]
$$

- $T$ : Discriminator의 layer의 수 
- $D^{l}$ : $l$ 번째 layer의 output feature map
- $N_{l}$ :$l$ 번째 layer의 feature의 수 



<br>

#### First stage full objectives

$$
\begin{aligned}
\min _{G, A, E, F, T} \max _{D} & \mathcal{L}_{\mathrm{mel}}+\lambda_{\mathrm{s} 2 \mathrm{s}} \mathcal{L}_{\mathrm{s} 2 \mathrm{s}}+\lambda_{\text {mono }} \mathcal{L}_{\text {mono }} \\
& +\lambda_{\mathrm{adv}} \mathcal{L}_{\mathrm{adv}}+\lambda_{\mathrm{fm}} \mathcal{L}_{\mathrm{fm}}
\end{aligned}
$$



<br><br>

### Second Stage

#### Duration prediction

Duration prediction을 학습하기 위해서 $L_{1}$ loss 사용 


$$
\mathcal{L}_{\text {dur }}=\mathbb{E}_{\boldsymbol{d}}\left[\left\|\boldsymbol{d}-\boldsymbol{d}_{\text {pred }}\right\|_{1}\right]
$$


- $\boldsymbol{d}$ : $\boldsymbol{a}_{\text {align }}$ 를 mel frame 축을 기준으로 더한 값을 사용 (ground truth)
- $\boldsymbol{d}_{\text {pred }}$ : 예측한 duration 값 ($$L\left(R\left(\boldsymbol{h}_{\text {text }}, \boldsymbol{s}\right)\right)$$)



<br><br>

#### Prosody prediction

Duration predictor는 다른 모듈과 독립적으로 학습되기 때문에 최적의 duration 값을 도출하지 못하거나 prosody predictor의 결과와 합쳐지 않는 결과를 줄 수 있음

Duration predictor가 최적의 값을 주지 못하더라도 데이터는 증강시켜 duration에 대하여 prosody invariance를 확보할 수 있도록 함



Original mel-spectogram에서 얻은 ground truth인 alignment, pitch, energy 를 사용하는 대신에 1-D bilinear interpolation을 이용하여 mel-spectogram을 시간축으로 줄이거나 늘려 증강된 sample $$\tilde{\boldsymbol{x}}$$ 을 얻음 

그 결과 음성의 속도는 바뀌더라도 pitch와 energy 커브는 변하지 않음








$$
\hat{\boldsymbol{x}}
$$
