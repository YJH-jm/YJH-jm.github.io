---
layout: single
title:  "PFLD: A Practical Facial Landmark Detector" # 글 제목
categories: 
  - Face Recognition # category
tags: 
  - [face landmark] # tag
toc: true # 오른쪽의 table of contents
toc_sticky: true # 스크롤 내려도 toc 고정
toc_label: 목차
author_profile: false
# sidebar: # 왼쪽 사이드 바
    # nav: "docs"
search: true # 글 검색 안됨
use_math: true
typore-root-url: ../

---

****



### PFLD: A Practical Facial Landmark Detector

2019

[논문](https://arxiv.org/pdf/1902.10859.pdf)

<br>

![image-20240312193954402](/images/2024-03-12-4/image-20240312193954402.png)

<br><br>

#### Introduction

Face landmark detection에서 주로 발생하는 어려움

##### Challenge # 1 Local Variation

표정, 빛, occlusion 가 있는 경우 face landmark가 잘못잡히거나 사라지는 경우 발생

##### Challenge # 2 Global Variation

포즈나 이미지 퀄리티는 이미지에서 얼굴에 영향을 미치는 주요한 요소

##### Challenge # 3  Data Imbalance

class 나 attributes (표정, 빛, occlusion 등)에 따라 데이터가 imbalance 함

##### Challenge # 4 \- Model Efficiency

모델 사이즈와 계산량이 중요한 제한 요소

##### 

PFLD는 실제 환경에서도 모바일 디바이스에서 실시간 (super real-time)으로 landmark detection 가능

**End-to-end single stage network** 를 사용

학습을 할 때만 하나의 보조 네트워크를 사용하여 각 샘플마다 **rotation information** 을 측정하여 성능을 높임

Geometrical regularization을 고려하여 만든 새로운 loss는 data imbalance 문제를 해결

- 각 상태에 따라서 샘플들의 weight 조정

Challenging benchmark에서 SOTA 성능이 나옴

모바일 폰에서 (Qualcomm ARM 845 processor) 2.1Mb 사이즈의 모델로 각 얼굴마다 140fps 로 작동



#### Methodology

![image-20240312200608862](/images/2024-03-12-4/image-20240312200608862.png)

- 보조 네트워크에서는 convolution layer와 fully-connected layer를 지나 rotation 정보가 output

- 메인 branch에서는 face landmark 정보가 output

<br>



##### Backbone Network

![image-20240312203020038](/images/2024-03-12-4/image-20240312203020038.png)

![image-20240312202444086](/images/2024-03-12-4/image-20240312202444086.png)

- MobileNet V2 이용
  - Depthwise Seperable Convolutions
  - Linear Bottlenecks
  - Inverted Residuals
  - ReLU6

- S1, S2, S3 같이 multi- scale로 구성하여 **Multi-Scale Fully-Connected (MS-FC)** 을 활용하여  얼굴의 global한 구조, 즉 전체적인 특징을 잘 잡아내서 높은 정확도
  - 사람의 얼굴은 전체적인 구조의 관계성이 있음

- 간단하지만 좋은 성능을 내는 구조
- Width parameter를 이용해서 모델 압축 가능
  - Width parameter인 width multiplier는 layer의 channel수를 일정 비율로 줄임



<br>



##### Auxiliary Network

기존의 특징점만 사용하는 모델보다 더 좋은 성능을 낼 수  있음

![image-20240312210637396](/images/2024-03-12-4/image-20240312210637396.png)



- Convolution연산과 fully-connected 연산으로 구성되어 있음 

- Backbone 의 4번째 block이 input으로 들어감

- 3차원 회전 정보인 **yaw**, **pitch**, and **roll** 각도를  측정하고자 함

  ![The-yaw-pitch-and-roll-angles](/images/2024-03-12-4/The-yaw-pitch-and-roll-angles.png){:.img-display }

  <center style="font-size:12px;">
      <a href="https://www.researchgate.net/figure/The-yaw-pitch-and-roll-angles-in-the-human-head-motion-11_fig1_305684696"> 출처 </a>
  </center>

  


Ground Truth 값과 predicted된 landmark로 오일러 각도를 계산해도 가능

하지만 초기에는 predicted 된 landmark 정확도가 좋지 않음

또한 학습 데이터에는 정면 얼굴이 없기 때문에 직접적으로 오일러 각도를 구할 수 없기 때문에 사용



<br>

오일러 각도를 계산하기 위한  추가적인 annotation을 사용하지 않음 

- yaw, pitch, roll 값이 얼마인지 annotation을 따로 하지 않음



<br>

학습 진행 방법 

1. 여러 장의 정면 이미지의 얼굴의 평균값을 이용하여 하나의 standard face를 만든 후 11개의 landmark 지정함
2. 각 얼굴과 standard face의 11개의 landmark의 rotation matrix 구함
3. Rotation matrix를 이용하여 오일러 각도 계산 



<br>



##### Loss Function

$$
L:= \frac{1}{M}\sum_{m=1}^{M}\sum_{n=1}^{N}\gamma_{n} \left\|d_{n}^{m}\right\|
$$


$$
L:= \frac{1}{M}\sum_{m=1}^{M}\sum_{n=1}^{N}\left (\sum_{c=1}^{C}{w_{n}^{c}}\sum_{k=1}^{K}{(1-cos{\theta_{n}^{k}})} \right )\left\|d_{n}^{m}\right\|
$$


- $M$ : input image의 개수
- $N$ : face landmark의 개수
- $C$ : attribute class 
- $K$ : 3d pose 정보의 수 (3- yaw, pitch, roll)
- $\gamma_{n} $: 3d pose 정보를 가짐
- $w_{n}^{c}$:  : data imbalance 해결하는 가중치

<br>

3D pose 정보와 2D landmark의 distance를 이용하여 간단하게 표현

Forward와 barkward 모두 간단하게 연산 가능

이 Loss function은 cascade 되기 때문에 single stage와 같은 효과를 주어 최적화 성능 높임

<br>

##### Implementation Details

Input image size : 112 x 112

Batch size : 256

 Optimizer : Adam

 Weight decay : 10e-6

 Momentum : 0.9

 Learning rate : 10e-4

 Nvidia GTX 1080Ti

<br>

#### Experimental Evaluation

##### Dataset

1. 300W
   - 5개의 얼굴 데이터를 annotation
     - -LFPW, AFW, HELEN, XM2VTS, IBUG
   -  68개 landmark
2. AFLW
   - 21개 landmark

<br>

#####  Experimental Results

###### Model Size & Processing Speed

![image-20240312214858029](/images/2024-03-12-4/image-20240312214858029.png)

- PFLD 뒤의 숫자는 depth 조절

<br>

###### Normalized Mean Error

![image-20240312220414575](/images/2024-03-12-4/image-20240312220414575.png){: .img-display}



<br>

Inter-pupil Normalization 


$$
L=dist(p_{96}, p_{97})
$$

$$
NME(S,S^{*})=\frac{1}{N}\sum_{i=1}^{N}\frac{\left\| s_{i}-s_{i}^{ * } \right\|_{2}^{2}}{L}
$$

- 눈동자 사이의 거리로 normalization

<br>

inter-ocular Normalization


$$
L=dist(p_{60}, p_{72})
$$

$$
NME(S,S^{*})=\frac{1}{N}\sum_{i=1}^{N}\frac{\left\| s_{i}-s_{i}^{ * } \right\|_{2}^{2}}{L}
$$

- 왼쪽 눈의 왼쪽 끝과 오른쪽 눈의 오른쪽 끝 좌표의 거리로 normalization



<br>

PFLD의 성능이 위의 두 요소에서 훨씬 좋음을 확인



<br><br>

![image-20240312223216894](/images/2024-03-12-4/image-20240312223216894.png)

<br>


$$
L=\sqrt{Width * Height}
$$

$$
NME(S,S^{*})=\frac{1}{N}\sum_{i=1}^{N}\frac{\left\| s_{i}-s_{i}^{ * } \right\|_{2}^{2}}{L}
$$

- 위의 표의 NME는 모든 특징점을 포함하는 box의 size로 normalization



<br>



#### Conclusion

accuracy, efficiency, compactness 모두 만족

Backbone network의 성능을 높이기 위한 auxiliary network 사용

Geometric regularization과 data imbalance를 모두 고려한 새로운 loss function 고안

auxiliary network 통해 Rotation information (yaw, pitch, roll) 을 사용하여 geometric 제약을 둠

- 정확도 향상

<br><br>
